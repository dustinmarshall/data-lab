{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERATE wb_ag_datasets.csv FILE ###\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "def is_related_to_food_or_agriculture(description):\n",
    "    keywords = {\n",
    "    'food', 'agriculture', 'farming', 'crops', 'livestock', 'harvest', \n",
    "    'horticulture', 'irrigation', 'fertilizer', 'farm', 'ranch', 'grain',\n",
    "    'vegetable', 'fruit', 'meat', 'dairy', 'poultry', 'aquaculture', 'agronomy',\n",
    "    'food security', 'rural development', 'agricultural economics', 'sustainable agriculture',\n",
    "    'agricultural policy', 'land use', 'agricultural trade', 'food supply', 'agroecology',\n",
    "    'agribusiness', 'agricultural technology', 'agricultural finance', 'agricultural investment',\n",
    "    'crop rotation', 'soil management', 'pest management', 'agricultural research', 'agricultural extension',\n",
    "    'food processing', 'food distribution', 'market access', 'subsistence agriculture', 'commercial agriculture',\n",
    "    'agricultural productivity', 'nutrition', 'food aid', 'agricultural innovation', 'climate change and agriculture',\n",
    "    'agricultural insurance', 'rural livelihoods', 'agricultural workers', 'agricultural supply chain',\n",
    "    'biofortification', 'food policy', 'agricultural sustainability', 'agroforestry'\n",
    "    }\n",
    "\n",
    "    if description is None:\n",
    "        return False\n",
    "\n",
    "    description_lower = description.lower()\n",
    "    for keyword in keywords:\n",
    "        if keyword in description_lower:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "dirname = os.getcwd()\n",
    "file_path = os.path.join(dirname, 'data/wb_datasets.json')\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "datasets = data['data']\n",
    "base_url = 'https://datacatalogapi.worldbank.org/ddhxext/DatasetView?dataset_unique_id='\n",
    "simplified_datasets = []\n",
    "\n",
    "for dataset in tqdm(datasets, desc=\"Processing datasets\"):\n",
    "    if dataset.get('source') == \"MICRODATA\":\n",
    "        continue\n",
    "    dataset_id = dataset.get('dataset_unique_id')\n",
    "    url = base_url + dataset_id\n",
    "    response = requests.get(url)\n",
    "    response_data = response.json()\n",
    "    name = response_data.get('name', 'None')\n",
    "    description = response_data.get('identification', {}).get('description', 'None')\n",
    "\n",
    "    if not is_related_to_food_or_agriculture(description):\n",
    "        continue\n",
    "\n",
    "    project_id = response_data.get('identification', {}).get('wb_project_reference', 'None')\n",
    "    resources = response_data.get('Resources', 'None')\n",
    "    \n",
    "    files = []\n",
    "    if resources != 'None':\n",
    "        for resource in resources:\n",
    "            file = {\n",
    "                'name': resource.get('name', 'None'),\n",
    "                'description': resource.get('description', 'None'),\n",
    "                'url': resource.get('url', 'None')\n",
    "            }\n",
    "            files.append(file)\n",
    "\n",
    "    simplified_dataset = {\n",
    "        'name': name,\n",
    "        'description': description,\n",
    "        'dataset_id': dataset_id,\n",
    "        'project_id': project_id,\n",
    "        'files': json.dumps(files)  # Convert list of files to a JSON string\n",
    "    }\n",
    "    simplified_datasets.append(simplified_dataset)\n",
    "\n",
    "output_file_path = os.path.join(dirname, 'data/wb_ag_datasets.csv')\n",
    "with open(output_file_path, mode='w', newline='', encoding='utf-8') as output_file:\n",
    "    writer = csv.DictWriter(output_file, fieldnames=['name', 'description', 'dataset_id', 'project_id', 'files'])\n",
    "    writer.writeheader()\n",
    "    for dataset in simplified_datasets:\n",
    "        writer.writerow(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERATE LIST OF VIDEO IDS FROM YOUTUBE CHANNEL ###\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Function to get summary text\n",
    "def get_summary(text):\n",
    "    # Load api key from env\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    client = OpenAI(\n",
    "        api_key=api_key,\n",
    "    )\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        print(\"Error: Text must be a string\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # Model can be changed if needed\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Summarize the following text in 40 tokens or less using the framing, 'A video on the World Bank's YouTube channel detailing {summary}'\"},\n",
    "                {\"role\": \"user\", \"content\": text}\n",
    "            ]\n",
    "        )\n",
    "        # Accessing the text in the correct format\n",
    "        return response.choices[0].message.content  # Adjusted to access .text attribute and strip any extra whitespace\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return None\n",
    "\n",
    "# Function to get video description\n",
    "def get_video_description(video_id):\n",
    "    # Load api key from env\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "    \n",
    "    # Build the YouTube API client\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Try to fetch the caption track list\n",
    "    try:\n",
    "        response = youtube.captions().list(\n",
    "            part='snippet',\n",
    "            videoId=video_id\n",
    "        ).execute()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching caption tracks: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Check if captions are available\n",
    "    if not response['items']:\n",
    "        print(\"No captions available for this video.\")\n",
    "        return None\n",
    "\n",
    "    # Try to fetch video details\n",
    "    try:\n",
    "        response = youtube.videos().list(\n",
    "            part='snippet',\n",
    "            id=video_id\n",
    "        ).execute()\n",
    "\n",
    "        # Check if the video exists and has a snippet\n",
    "        if not response['items']:\n",
    "            print(\"Video not found.\")\n",
    "            return None\n",
    "\n",
    "        video_details = response['items'][0]['snippet']\n",
    "\n",
    "        # Return the description\n",
    "        return video_details['description']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Function to get all video ids from a channel\n",
    "def get_all_video_ids(channel_id):\n",
    "    # Load api key from env\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "    # Initialize list\n",
    "    video_ids = []\n",
    "\n",
    "    # Get the channel's content details\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    channel_response = youtube.channels().list(\n",
    "        id=channel_id,\n",
    "        part='contentDetails'\n",
    "    ).execute()\n",
    "\n",
    "    # Get the playlist ID for the channel's videos\n",
    "    playlist_id = channel_response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "\n",
    "    # Get videos from the playlist\n",
    "    next_page_token = None\n",
    "    while True:\n",
    "        playlist_response = youtube.playlistItems().list(\n",
    "            playlistId=playlist_id,\n",
    "            part='contentDetails',\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        ).execute()\n",
    "\n",
    "        video_ids += [item['contentDetails']['videoId'] for item in playlist_response['items']]\n",
    "        \n",
    "        # Check if there is a next page\n",
    "        next_page_token = playlist_response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return video_ids\n",
    "\n",
    "# Example\n",
    "channel_id = 'UCE9mrcoX-oE-2f1BL-iPPoQ' # World Bank's Channel ID\n",
    "video_ids = get_all_video_ids(channel_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERATE EMBEDDINGS DICTIONARY FROM ALL VIDEOS ON YOUTUBE CHANNEL ###\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Group transcript entries (~10 words each) into groups of 20 with 20% overlap\n",
    "def group_transcript_entries(transcript):\n",
    "    grouped_transcripts = []\n",
    "    group_size = 20\n",
    "    overlap = 4\n",
    "\n",
    "    for i in range(0, len(transcript), group_size - overlap):\n",
    "        group = transcript[i:i + group_size]\n",
    "        group_text = \" \".join([entry['text'] for entry in group])\n",
    "        grouped_transcripts.append((group[0]['start'], group_text))\n",
    "\n",
    "    return grouped_transcripts\n",
    "\n",
    "# Format grouped transcripts into a list\n",
    "def format_transcript_to_list(grouped_transcripts, video_id, summary):\n",
    "    formatted_transcript = []\n",
    "    \n",
    "    for start_time, group_text in grouped_transcripts:\n",
    "        formatted_transcript.append({\"excerpt link\" : f\"https://www.youtube.com/watch?v={video_id}&t={int(start_time)}s\", \n",
    "                                     \"transcript excerpt\" : group_text,\n",
    "                                     \"video summary\" : summary})\n",
    "\n",
    "    return formatted_transcript\n",
    "\n",
    "# Extract transcipt\n",
    "def get_video_transcript(video_id):\n",
    "    # Load api key from env\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "    \n",
    "    # Build the YouTube API client\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Try to fetch the caption track list\n",
    "    try:\n",
    "        response = youtube.captions().list(\n",
    "            part='snippet',\n",
    "            videoId=video_id\n",
    "        ).execute()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching caption tracks: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Check if captions are available\n",
    "    if not response['items']:\n",
    "        print(\"No captions available for this video.\")\n",
    "        return None\n",
    "\n",
    "    # Extracting the transcript using the youtube_transcript_api\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "        # Grouping the transcript entries\n",
    "        grouped_transcripts = group_transcript_entries(transcript)\n",
    "        # Get video description to summarize\n",
    "        video_description = get_video_description(video_id)\n",
    "        # Get the summary\n",
    "        summary = get_summary(video_description)\n",
    "        # Formatting the grouped transcript to dictionary\n",
    "        formatted_transcript = format_transcript_to_list(grouped_transcripts, video_id, summary)\n",
    "        return formatted_transcript\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching transcript: {e}\")\n",
    "        return None\n",
    "\n",
    "# generate the transcripts list\n",
    "transcripts_list = []\n",
    "\n",
    "for id in tqdm(video_ids):\n",
    "    transcript_list = get_video_transcript(id)\n",
    "    if transcript_list:\n",
    "        transcripts_list += transcript_list\n",
    "\n",
    "# Saving the list to a new JSON file\n",
    "dirname = os.getcwd()\n",
    "output_file_path = os.path.join(dirname, 'data/wb_youtube_videos_complete.json')\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    json.dump(transcripts_list, output_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE wb_ag_projects_df FROM WB PROJECTS AND DOCUMENTS API ###\n",
    "\n",
    "# Function to create wb_ag_projects_df from WB Projects and Documents API\n",
    "def create_ag_projects_df():\n",
    "    \n",
    "    print(\"creating wb_ag_projects_df from WB Projects API...\")\n",
    "    wb_ag_projects = requests.get('https://search.worldbank.org/api/v2/projects?rows=10000&mjsectorcode_exact=AX').json()['projects']\n",
    "    wb_ag_projects_df = pd.DataFrame.from_dict(wb_ag_projects, orient='index')\n",
    "    \n",
    "    print(\"adding projectdocs from to WB Documents API wb_ag_projects_df...\")\n",
    "    for index, row in tqdm(wb_ag_projects_df.iterrows(), total=wb_ag_projects_df.shape[0]):\n",
    "        wb_ag_projects_df.at[index, 'projectdocs'] = {}\n",
    "        response = requests.get(f\"https://search.worldbank.org/api/v2/wds?format=json&fl=pdfurl,docty&proid={row['id']}\").json()\n",
    "        if response:\n",
    "            for doc in reversed(response['documents'].values()):\n",
    "                if 'docty' in doc and 'pdfurl' in doc:\n",
    "                    doctype = doc['docty']\n",
    "                    pdfurl = doc['pdfurl']\n",
    "                    wb_ag_projects_df.at[index, 'projectdocs'][doctype] = pdfurl\n",
    "\n",
    "    print(\"saving wb_ag_projects_df to csv...\")\n",
    "    wb_ag_projects_df.to_csv('data/wb_ag_projects.csv', index=False)\n",
    "\n",
    "# Check if wb_ag_projects.csv exists, if not create it\n",
    "if not os.path.exists('data/wb_ag_projects.csv'):\n",
    "    create_ag_projects_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERATE ASSISTANT FOR GENERATING USE CASES ###\n",
    "\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "\n",
    "csv_input = 'data/wb_ag_ext_projects.csv'\n",
    "csv_output = 'data/wb_ag_ext_usecases.csv'\n",
    "\n",
    "# Function to submit tool outputs\n",
    "def submit_tool_outputs(thread_id, run_id, tool_call_id, output):\n",
    "    client.beta.threads.runs.submit_tool_outputs(\n",
    "        thread_id=thread_id,\n",
    "        run_id=run_id,\n",
    "        tool_outputs=[{\n",
    "            \"tool_call_id\": tool_call_id,\n",
    "            \"output\": json.dumps(output)\n",
    "        }]\n",
    "    )\n",
    "\n",
    "# Function to process a document and update the DataFrame\n",
    "def process_document(url, id, project, implementer, region, country, documents, sectors, years, contacts):\n",
    "    \n",
    "    # Load api key from env\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    # Read wb_use_case_summaries.csv into wb_use_case_summaries_df\n",
    "    if os.path.exists(csv_output):\n",
    "        use_cases_df = pd.read_csv(csv_output)\n",
    "    else:\n",
    "        use_cases_df = pd.DataFrame(columns=['id', 'use_case', 'project', 'description', 'implementer', 'region', 'country', 'documents', 'sectors', 'years', 'contacts'])\n",
    "\n",
    "    # Download the file\n",
    "    response = requests.get(url)\n",
    "    file_name = 'downloaded_file.pdf'\n",
    "    with open(file_name, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "    # Initialize OpenAI client\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    # Upload the file to OpenAI\n",
    "    print(\"Uploading files...\")\n",
    "    file_path = file_name\n",
    "    project_document_file = client.files.create(\n",
    "      file=open(file_path, \"rb\"),\n",
    "      purpose='assistants'\n",
    "    )\n",
    "    \n",
    "    # Initialize tools\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"retrieval\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Function mapping\n",
    "    function_mapping = {\n",
    "    }\n",
    "    \n",
    "    # Create the Assistant\n",
    "    print(\"Creating Assistant...\")\n",
    "    assistant = client.beta.assistants.create(\n",
    "        name=\"Use Case Summarizer\",\n",
    "        instructions=\n",
    "            '''\n",
    "            Role: \n",
    "            In your knowledge base is a pdf file detailing an agricultural project. Your job is to identify the distinct methodologies that\n",
    "            the project leverages to implement the project, then produce a summary of each methodology (framing the methodology as a \n",
    "            use case example) in alignment with the provided template. The goal is to produce use case examples that can be used by other \n",
    "            teams in their own projects.\n",
    "\n",
    "            Instructions:\n",
    "            - Review the document in your knowledge base and identify the distinct methodologies that the project leverages.\n",
    "            - For each, generate a 200 word description and 5-10 word title detailing how the use case example was implemented.\n",
    "            - Return the title and description in a JSON array, with no additional text before or after.\n",
    "            - Ensure that the description is actually 200 words and the title is actually 5-10 words.\n",
    "            - Even if methodologies aren't explicitly mentioned, do your best to deduce them, otherwise return an empty JSON array.\n",
    "            \n",
    "            Template:\n",
    "            [\n",
    "                {\n",
    "                    \"use_case\": \"Title of use case example 1\",\n",
    "                    \"description\": \"Description of use case example 1\"\n",
    "                },\n",
    "                {\n",
    "                    \"use_case\": \"Title of use case example 2\",\n",
    "                    \"description\": \"Description of use case example 2\"\n",
    "                }\n",
    "            ]\n",
    "            ''',\n",
    "        model=\"gpt-4-0125-preview\",\n",
    "        tools = tools,\n",
    "        file_ids=[project_document_file.id]\n",
    "    )\n",
    "\n",
    "    # Create a thread\n",
    "    print(\"Creating thread...\")\n",
    "    thread = client.beta.threads.create(\n",
    "    )\n",
    "\n",
    "    # Add message to the thread\n",
    "    print(\"Adding message to the thread...\")\n",
    "    message = client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content= \"\"\n",
    "    )\n",
    "\n",
    "    # Run the Assistant\n",
    "    print(\"Running the Assistant...\")\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id, \n",
    "        assistant_id=assistant.id,\n",
    "        instructions=\"\"\n",
    "    )\n",
    "\n",
    "    # Handle tool outputs\n",
    "    while run.status != 'completed':\n",
    "        time.sleep(10)\n",
    "        \n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id\n",
    "        )\n",
    "        \n",
    "        # Print the run status\n",
    "        print(f\"Run status: {run.status}\")\n",
    "        \n",
    "        # If the run is failed, retry\n",
    "        if run.status == \"failed\":\n",
    "            print(\"Run failed:\", run.last_error)\n",
    "            break\n",
    "\n",
    "        # Handle the run status\n",
    "        if run.status == \"requires_action\":\n",
    "            print(\"Action required by the assistant...\")\n",
    "            for tool_call in run.required_action.submit_tool_outputs.tool_calls:\n",
    "                if tool_call.type == \"function\":\n",
    "                    function_name = tool_call.function.name\n",
    "                    print(f\"Function name: {function_name}\")\n",
    "                    arguments = json.loads(tool_call.function.arguments)\n",
    "                    print(f\"Arguments: {arguments}\")\n",
    "                    if function_name in function_mapping:\n",
    "                        print(f\"Calling function {function_name}...\")\n",
    "                        response = function_mapping[function_name](**arguments)\n",
    "                        submit_tool_outputs(thread.id, run.id, tool_call.id, response)\n",
    "\n",
    "    # Fetch the Assistant's response  \n",
    "    print(\"Fetching Assistant's response...\")\n",
    "    reply = client.beta.threads.messages.list(\n",
    "        thread_id=thread.id\n",
    "    )\n",
    "    messages = reply.data\n",
    "\n",
    "    assistant_reply = \"\"\n",
    "    for message in messages:\n",
    "        if message.role == \"assistant\":\n",
    "            for content in message.content:\n",
    "                if content.type == \"text\":\n",
    "                    assistant_reply = content.text.value\n",
    "                    print(\"assistant_reply: \", assistant_reply, \"type: \", type(assistant_reply))\n",
    "                    # Find the position of the first '[' and the last ']'\n",
    "                    start_index = assistant_reply.find('[')\n",
    "                    end_index = assistant_reply.rfind(']')\n",
    "\n",
    "                    # Extract the substring between these positions\n",
    "                    if start_index != -1 and end_index != -1:\n",
    "                        json_string = assistant_reply[start_index:end_index+1]\n",
    "                        try:\n",
    "                            formatted_assistant_reply = json.loads(json_string)\n",
    "                            print(\"formatted_assistant_reply: \", formatted_assistant_reply, \"type: \", type(formatted_assistant_reply))\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(\"Failed to decode JSON. Error: \", e)\n",
    "                    else:\n",
    "                        print(\"The string does not contain a valid JSON structure.\")\n",
    "\n",
    "                    break\n",
    "            if assistant_reply:\n",
    "                break\n",
    "    \n",
    "    # Process the Assistant's response\n",
    "    for use_case in formatted_assistant_reply:\n",
    "        \n",
    "        print(\"use case: \", use_case, \"type: \", type(use_case))\n",
    "        \n",
    "        # Check if the use_case is a string and convert it to a dictionary\n",
    "        if isinstance(use_case, str):\n",
    "            use_case = ast.literal_eval(use_case)\n",
    "        \n",
    "        # Add additional fields to the use_case\n",
    "        use_case['id'] = id\n",
    "        use_case['project'] = project\n",
    "        use_case['implementer'] = implementer\n",
    "        use_case['region'] = region\n",
    "        use_case['country'] = country\n",
    "        use_case['documents'] = documents\n",
    "        use_case['sectors'] = sectors\n",
    "        use_case['years'] = years\n",
    "        use_case['contacts'] = contacts\n",
    "\n",
    "        # Add new row to DataFrame\n",
    "        use_cases_df = pd.concat([use_cases_df, pd.DataFrame([use_case])], ignore_index=True)\n",
    "\n",
    "        # Write the updated DataFrame to CSV\n",
    "        use_cases_df.to_csv(csv_output, index=False)\n",
    "\n",
    "    # Delete the file locally\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "    else:\n",
    "        print(\"The file does not exist\")\n",
    "        \n",
    "    ## Delete the file object\n",
    "    #client.beta.assistants.files.delete(\n",
    "    #    file_id=project_document_file.id,\n",
    "    #    assistant_id=assistant.id\n",
    "    #)\n",
    "        \n",
    "    ## Delete the assistant\n",
    "    #client.beta.assistants.delete(\n",
    "    #    assistant_id=assistant.id\n",
    "    #)\n",
    "\n",
    "# Read wb_ag_projects.csv into wb_ag_projects_df\n",
    "projects_df = pd.read_csv(csv_input)\n",
    "\n",
    "# Read wb_use_case_summaries.csv into wb_use_case_summaries_df\n",
    "if os.path.exists(csv_output):\n",
    "    use_cases_df = pd.read_csv(csv_output)\n",
    "else:\n",
    "    use_cases_df = pd.DataFrame(columns=['id', 'use_case', 'project', 'description', 'implementer', 'region', 'country', 'documents', 'sectors', 'years', 'contacts'])\n",
    "\n",
    "# iterate through wb_ag_projects_df and process each document\n",
    "for index, row in projects_df.iterrows():\n",
    "    if not use_cases_df['id'].isin([row['id']]).any():\n",
    "        # Check if projectdocs is a string and convert it to a dictionary\n",
    "        if isinstance(row['documents'], str):\n",
    "            try:\n",
    "                documents = ast.literal_eval(row['documents'])\n",
    "            except ValueError:\n",
    "                # Handle the exception if the string cannot be converted to a dictionary\n",
    "                continue\n",
    "        for doctype, url in documents.items():\n",
    "            if doctype == 'Project Paper' or doctype == 'Implementation Completion and Results Report' or doctype == 'Implementation Completion Report Review': \n",
    "                print(\"Processing document: \", doctype, \" for project: \", row['id'])\n",
    "                process_document(url, row['id'], row['project'], row['implementer'], row['region'], row['country'], row['documents'], row['sectors'], row['years'], row['contacts'])\n",
    "                #time.sleep(300)\n",
    "                break\n",
    "            elif doctype == 'Project Appraisal Document' or doctype == 'Project Information Document':\n",
    "                print(\"Processing document: \", doctype, \" for project: \", row['id'])\n",
    "                process_document(url, row['id'], row['project'], row['implementer'], row['region'], row['country'], row['documents'], row['sectors'], row['years'], row['contacts'])\n",
    "                #time.sleep(300)\n",
    "                break\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### DELETE ASSISTANTS AND FILES ###\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      5\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "### DELETE ASSISTANTS AND FILES ###\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# Load api key from env\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "my_assistants = client.beta.assistants.list(\n",
    "    limit=\"100\"\n",
    ")\n",
    "\n",
    "# Delete all assistants and files\n",
    "for assistant in my_assistants:\n",
    "    \n",
    "    # # Delete all files\n",
    "    try:\n",
    "        my_files = client.beta.assistants.files.list(\n",
    "            \n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching files list:\", e)\n",
    "        my_files = []\n",
    "    for file in my_files:\n",
    "        print(\"Deleting file ID:\", file.id)\n",
    "        try:\n",
    "            client.beta.assistants.files.delete(\n",
    "                file_id=file.id,\n",
    "                assistant_id=file.assistant_id\n",
    "            )\n",
    "            print(\"File deleted successfully.\")\n",
    "        except Exception as e:\n",
    "            print(\"Error deleting file:\", e)\n",
    "    \n",
    "    # Delete the assistant\n",
    "    try:\n",
    "        client.beta.assistants.delete(\n",
    "            assistant_id=assistant.id\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting the index...\n",
      "Creating the index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing use case examples...: 100%|██████████| 50/50 [00:20<00:00,  2.48it/s]\n",
      "Processing learning materials...: 100%|██████████| 25/25 [00:12<00:00,  2.05it/s]\n",
      "Processing datasets...: 100%|██████████| 25/25 [00:13<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "### GENERATE PINECONE VECTOR DATABASE AND EMBEDDINGS FOR USE CASES ###\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import streamlit as st\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import time\n",
    "import ast\n",
    "\n",
    "# Load api key from env\n",
    "pinecone_api_key = st.secrets[\"PINECONE_API_KEY\"]\n",
    "api_key = st.secrets[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Delete the index\n",
    "print(\"Deleting the index...\")\n",
    "pc.delete_index(\"agrifooddatalab-index\")\n",
    "\n",
    "# Create the index\n",
    "print(\"Creating the index...\")\n",
    "pc.create_index(\n",
    "    name=\"agrifooddatalab-index\",\n",
    "    dimension=1536,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(\n",
    "        cloud='aws', \n",
    "        region='us-west-2'\n",
    "    ) \n",
    ") \n",
    "\n",
    "# Connect to the index\n",
    "index = pc.Index(\"agrifooddatalab-index\")\n",
    "\n",
    "# Function to get embeddings\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    try:\n",
    "        return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Load wb_ag_ext_usecases.csv file\n",
    "dirname = os.getcwd()\n",
    "file_path = os.path.join(dirname, 'data/wb_ag_usecases.csv')\n",
    "\n",
    "with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for i, entry in enumerate(tqdm(list(reader)[:50], desc=\"Processing use case examples...\")):\n",
    "        # Extracting the text\n",
    "        embedding_text = f\"Title: {entry['use_case']}.\\nDescription: {entry['description']}\"\n",
    "        # Get the embedding\n",
    "        embedding = get_embedding(embedding_text)\n",
    "\n",
    "        # Try to upsert up to three times with a wait time between retries\n",
    "        max_retries = 3\n",
    "        wait_time = 5  # wait time in seconds\n",
    "\n",
    "        # generate the use case id, starting with U00001\n",
    "        use_case_id = \"U\" + f'{i+1:05}'\n",
    "                \n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                # Insert the embedding into the index\n",
    "                index.upsert([\n",
    "                    (\n",
    "                        use_case_id, \n",
    "                        embedding, \n",
    "                        {\n",
    "                            'title': entry['use_case'],\n",
    "                            'description': entry['description'],\n",
    "                            'type': 'use case',\n",
    "                            'project': entry['project'],\n",
    "                            'implementer': entry['implementer'],\n",
    "                            'region': entry['region'],\n",
    "                            'country': entry['country'],\n",
    "                            'document(s)': ast.literal_eval(entry['documents']),\n",
    "                            'subtopic(s)': ast.literal_eval(entry['sectors']),\n",
    "                            'year(s)': ast.literal_eval(entry['years']),\n",
    "                            'contact(s)': ast.literal_eval(entry['contacts']),\n",
    "                            'project_id': entry['id'],\n",
    "                        }\n",
    "                    )\n",
    "                ])\n",
    "                break  # If upsert is successful, break out of the retry loop\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt} failed: {e}\")\n",
    "                if attempt < max_retries:\n",
    "                    print(f\"Waiting for {wait_time} seconds before retrying...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(\"Max retries reached. Moving to the next item.\")\n",
    "             \n",
    "# Load wb_ag_ext_papers.csv file       \n",
    "file_path = os.path.join(dirname, 'data/wb_ag_ext_papers.csv')\n",
    "\n",
    "with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for entry in tqdm(list(reader)[:25], desc=\"Processing learning materials...\"):\n",
    "        # Extracting the text\n",
    "        embedding_text = f\"Title: {entry['document']}.\\nDescription: {entry['abstract']}\"\n",
    "        # Get the embedding\n",
    "        embedding = get_embedding(embedding_text)\n",
    "\n",
    "        # Try to upsert up to three times with a wait time between retries\n",
    "        max_retries = 3\n",
    "        wait_time = 5  # wait time in seconds\n",
    "\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                # Insert the embedding into the index\n",
    "                index.upsert([\n",
    "                    (\n",
    "                        entry['id'], \n",
    "                        embedding, \n",
    "                        {\n",
    "                            'title': entry['document'],\n",
    "                            'description': entry['abstract'],\n",
    "                            'type': 'learning',\n",
    "                            'date': entry['date'],\n",
    "                            'author(s)': ast.literal_eval(entry['authors']),\n",
    "                            'sector(s)': ast.literal_eval(entry['sectors']),\n",
    "                            'implementer': entry['implementer'],\n",
    "                            'url': entry['url']\n",
    "                        }\n",
    "                    )\n",
    "                ])\n",
    "                break  # If upsert is successful, break out of the retry loop\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt} failed: {e}\")\n",
    "                if attempt < max_retries:\n",
    "                    print(f\"Waiting for {wait_time} seconds before retrying...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(\"Max retries reached. Moving to the next item.\")\n",
    "                    \n",
    "# Load wb_ag_ext_datasets.csv file       \n",
    "file_path = os.path.join(dirname, 'data/wb_ag_datasets.csv')\n",
    "\n",
    "with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for entry in tqdm(list(reader)[:25], desc=\"Processing datasets...\"):\n",
    "        # Extracting the text\n",
    "        embedding_text = f\"Title: {entry['name']}.\\nDescription: {entry['description']}\"\n",
    "        # Get the embedding\n",
    "        embedding = get_embedding(embedding_text)\n",
    "\n",
    "        # Try to upsert up to three times with a wait time between retries\n",
    "        max_retries = 3\n",
    "        wait_time = 5  # wait time in seconds\n",
    "\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                # Insert the embedding into the index\n",
    "                index.upsert([\n",
    "                    (\n",
    "                        entry['dataset_id'], \n",
    "                        embedding, \n",
    "                        {\n",
    "                            'title': entry['name'],\n",
    "                            'description': entry['description'],\n",
    "                            'type': 'dataset',\n",
    "                            'project_id': entry['project_id'],\n",
    "                            'file(s)': ast.literal_eval(entry['files'])\n",
    "                        }\n",
    "                    )\n",
    "                ])\n",
    "                break  # If upsert is successful, break out of the retry loop\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt} failed: {e}\")\n",
    "                if attempt < max_retries:\n",
    "                    print(f\"Waiting for {wait_time} seconds before retrying...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(\"Max retries reached. Moving to the next item.\")\n",
    "\n",
    "# Example: Querying the index\n",
    "# query_result = index.query(queries=[[0.1, 0.2, ..., 0.128]], top_k=5)\n",
    "\n",
    "# Remember to delete the index if it is no longer needed\n",
    "#pinecone.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ukraine',\n",
       " 'Kosovo',\n",
       " 'Mauritius',\n",
       " 'Tunisia',\n",
       " 'Western and Central Africa',\n",
       " 'Chad',\n",
       " 'Philippines',\n",
       " 'Lebanon',\n",
       " 'Afghanistan',\n",
       " 'Ghana',\n",
       " 'Central African Republic',\n",
       " 'Turkiye',\n",
       " 'Kazakhstan',\n",
       " 'Morocco',\n",
       " 'China',\n",
       " 'Moldova',\n",
       " 'Eastern and Southern Africa']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import ast\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/wb_ag_usecases.csv')\n",
    "\n",
    "# Extract the unique implementers from the 'implementer' column\n",
    "unique_implementers = df['implementer'].unique().tolist()\n",
    "unique_implementers\n",
    "\n",
    "# Extract the unique regions from the 'region' column\n",
    "unique_regions = df['region'].unique().tolist()\n",
    "unique_regions\n",
    "\n",
    "# Extract the unique years from the 'years' column\n",
    "df['years'] = df['years'].apply(ast.literal_eval)\n",
    "unique_years = df['years'].explode().unique().tolist()\n",
    "unique_years\n",
    "\n",
    "# Extract the unique sectors from the lists of sectors in the 'sectors' column\n",
    "df['sectors'] = df['sectors'].apply(ast.literal_eval)\n",
    "unique_sectors = df['sectors'].explode().unique().tolist()\n",
    "unique_sectors\n",
    "\n",
    "# Extract the unique countries from the 'country' column\n",
    "unique_countries = df['country'].unique().tolist()\n",
    "unique_countries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'asst_kHQeG5D5I49LnuBFlyl0NzUj',\n",
       " 'created_at': 1708358707,\n",
       " 'description': None,\n",
       " 'file_ids': [],\n",
       " 'instructions': \"Use the conversation history to call the download_file_upload_to_assistant function, then use the code_interpreter or retrieval tool to analyze the file based on the user's requests. Responses should be informative, but very concise; keep sentences and paragraphs short. End every response with a follow-up question that invites the user to analyze the document further.\",\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-3.5-turbo-0125',\n",
       " 'name': 'AgriFood Data Lab',\n",
       " 'object': 'assistant',\n",
       " 'tools': [{'function': {'name': 'download_file_upload_to_assistant',\n",
       "    'description': 'Download a file from a given URL and upload it to the assistant.',\n",
       "    'parameters': {'type': 'object',\n",
       "     'properties': {'url': {'type': 'string',\n",
       "       'description': 'The url of the file to download.'}},\n",
       "     'required': ['url']}},\n",
       "   'type': 'function'},\n",
       "  {'type': 'code_interpreter'},\n",
       "  {'type': 'retrieval'}]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from pinecone import Pinecone\n",
    "\n",
    "def show_json(obj):\n",
    "    display(json.loads(obj.model_dump_json()))\n",
    "\n",
    "client = OpenAI(api_key=\"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\")\n",
    "pc = Pinecone(api_key=\"9ad107bb-ea72-42f7-87ab-b0f55db4d98c\", environment=\"gcp-starter\")\n",
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo-0125\"\n",
    "\n",
    "# Function to download file and upload to OpenAI assistant\n",
    "#@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def download_file_upload_to_assistant(file_url):\n",
    "    \n",
    "    # Download the file\n",
    "    try:\n",
    "        response = requests.get(file_url)\n",
    "        with open('downloaded_file.pdf', 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(\"File downloaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "    # Upload the file to OpenAI\n",
    "    try:\n",
    "        client.files.create(\n",
    "            file=open('downloaded_file.pdf', \"rb\"),\n",
    "            purpose='assistants'\n",
    "        )\n",
    "        print(\"File uploaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "assistant_tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"download_file_upload_to_assistant\",\n",
    "            \"description\": \"Download a file from a given URL and upload it to the assistant.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"url\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The url of the file to download.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"url\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"code_interpreter\"\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"retrieval\",\n",
    "    }\n",
    "]\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"AgriFood Data Lab\",\n",
    "    instructions=\"Use the conversation history to call the download_file_upload_to_assistant function, then use the code_interpreter or retrieval tool to analyze the file based on the user's requests. Responses should be informative, but very concise; keep sentences and paragraphs short. End every response with a follow-up question that invites the user to analyze the document further.\",\n",
    "    model=GPT_MODEL,\n",
    "    tools=assistant_tools\n",
    ")\n",
    "show_json(assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'thread_YlTNakRKXgEUxWtDSXg7NwMo',\n",
       " 'created_at': 1708358711,\n",
       " 'metadata': {},\n",
       " 'object': 'thread',\n",
       " 'tool_resources': []}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Assistant: Welcome to the AgriFood Data Lab! Explore agricultural use cases, datasets, and learning resources, with AI-enabled search, retrieval, and analysis capabilities. How can we help you today?  \\n  \\nUser: I'm starting a new project and I need to find some use cases to help me get started. Can you help me with that?  \\n  \\nAssistant: Could you describe what you're looking for to us in more detail?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"I'm looking for use case examples related to food security in Ghana.  \\n  \\nAssistant: Thank you. We've added some optional filters that you can edit to help us narrow down your search. We used the conversation and selected filters to run an AI-enabled semantic search on our database. Here are the top matches:  \\n  \\n**Food Market Integration and Value Chain Development in Western and Central Africa:** This use case example emphasizes the importance of integrating regional food markets and developing strategic value chains for enhancing food security. (ID: U00010)  \\n  \\n**Enhancing Social Resilience in Ghana:** This use case example discusses the strategy implemented to enhance social resilience in Ghana by addressing gender inequities in agriculture. (ID: U00028)  \\n  \\n**Digital Advisory and Monitoring in Western and Central Africa:** This use case example showcases the integration of digital advisory services into regional agriculture. (ID: U00008)  \\n  \\n**Integrated Landscape Management in Western and Central Africa:** This use case example focuses on Strengthening Regional Food Security through Integrated Landscape Management (ILM) as a part of Sierra Leone's Food System Resilience Program. (ID: U00013)  \\n  \\n**Agricultural Productivity and Livestock Health Enhancement in Central African Republic:** This use case example outlines the methodological approach taken in the Central African Republic (CAR) Emergency Food Crisis Response Project to enhance agricultural productivity and livestock health to increase food production and improve the resilience of targeted smallholder farmers and food insecure households. (ID: U00030)  \\n  \\nWould you like to know more about any of these matches or search for something else?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"I'm interested in the first use case example. Can you provide me with more details?  \\n  \\n Assistant: Here's all of the information we have on that record in our database:  \\n  \\n**Title:** Food Market Integration and Value Chain Development  \\n**Description:** This use case example emphasizes the importance of integrating regional food markets and developing strategic value chains for enhancing food security. The project built upon activities from the parent initiative by disseminating market information and investing in market infrastructure. An innovative strategy was to extend the project’s reach beyond the originally targeted value chains (poultry, rice, and soybeans) to include vegetables, cowpeas, and roots and tubers, which are crucial for local nutrition and livelihoods. The activities aimed to facilitate trade across key corridors and consolidate the food reserve system. This involved scaling up support for the dissemination of market information to help farmers access local, regional, and international markets, and establishing productive alliances between producers and aggregators for better commercialization of agricultural products. Additionally, the project aimed to enhance storage infrastructure and adopt post-harvest processing technologies through matching grants. The focus on promoting standards for quality and packaging aimed to meet market demands, benefiting small and medium-sized enterprises (SMEs), women and youth-led producer groups, and cooperatives within the targeted value chains.  \\n**Type:** use case  \\n**Project:** Additional Financing to West Africa Food Systems Resilience Program, Phase 1  \\n**Implementer:** Ministry of Agriculture - Niger, Ministry of Agriculture, Hydro-Agricultural Developments and Mechanization - Burkina Faso, Ministry of Rural Development - Mali, Ministry of Agriculture, Livestock and Rural Development - Togo  \\n**Region:** Western and Central Africa  \\n**Country:** Western and Central Africa  \\n**Document(s):**  \\n - [Environmental and Social Review Summary] (http://documents.worldbank.org/curated/en/099071723120011453/pdf/P18113901889070520a3150583a67023157.pdf)  \\n - [Project Information Document](http://documents.worldbank.org/curated/en/099071123185032795/pdf/P1811390d3d36e0508ba40ba61471fc83a.pdf)  \\n - [Environmental and Social Commitment Plan](http://documents.worldbank.org/curated/en/099071723115524734/pdf/P18113908c7c400208c8a0667074738849.pdf)  \\n**Subtopic(s):** Agricultural markets, commercialization and agri-business, Agricultural Extension, Research, and Other Support Activities, Crops, Livestock, Public Administration - Agriculture, Fishing & Forestry  \\n**Year(s):** 2023  \\n**Contact(s):** Katie Kennedy Freeman, Erick Herman Abiassi  \\n**Project ID:** P181139  \\n  \\nWould you like use to analyze any of the linked files or search for something else?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Can you analyze the Environmental and Social Review Summary for me?\"\n",
    "    }   \n",
    "]\n",
    "thread = client.beta.threads.create()\n",
    "for message in messages:\n",
    "    client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=message[\"role\"],\n",
    "        content=message[\"content\"]\n",
    "    )\n",
    "show_json(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'run_8XzG8P06jmuvGq3tcT2VzdOv',\n",
       " 'assistant_id': 'asst_kHQeG5D5I49LnuBFlyl0NzUj',\n",
       " 'cancelled_at': None,\n",
       " 'completed_at': None,\n",
       " 'created_at': 1708358714,\n",
       " 'expires_at': 1708359314,\n",
       " 'failed_at': None,\n",
       " 'file_ids': [],\n",
       " 'instructions': \"Use the conversation history to call the download_file_upload_to_assistant function, then use the code_interpreter or retrieval tool to analyze the file based on the user's requests. Responses should be informative, but very concise; keep sentences and paragraphs short. End every response with a follow-up question that invites the user to analyze the document further.\",\n",
       " 'last_error': None,\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-3.5-turbo-0125',\n",
       " 'object': 'thread.run',\n",
       " 'required_action': None,\n",
       " 'started_at': None,\n",
       " 'status': 'queued',\n",
       " 'thread_id': 'thread_YlTNakRKXgEUxWtDSXg7NwMo',\n",
       " 'tools': [{'function': {'name': 'download_file_upload_to_assistant',\n",
       "    'description': 'Download a file from a given URL and upload it to the assistant.',\n",
       "    'parameters': {'type': 'object',\n",
       "     'properties': {'url': {'type': 'string',\n",
       "       'description': 'The url of the file to download.'}},\n",
       "     'required': ['url']}},\n",
       "   'type': 'function'},\n",
       "  {'type': 'code_interpreter'},\n",
       "  {'type': 'retrieval'}],\n",
       " 'usage': None}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    ")\n",
    "show_json(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'requires_action'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def wait_on_run(run, thread):\n",
    "    while run.status == \"queued\" or run.status == \"in_progress\":\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id,\n",
    "        )\n",
    "        time.sleep(0.5)\n",
    "    return run\n",
    "run = wait_on_run(run, thread)\n",
    "run.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Name: download_file_upload_to_assistant\n",
      "Function Arguments: {'url': 'http://documents.worldbank.org/curated/en/099071723120011453/pdf/P18113901889070520a3150583a67023157.pdf'}\n"
     ]
    }
   ],
   "source": [
    "# Extract single tool call\n",
    "tool_call = run.required_action.submit_tool_outputs.tool_calls[0]\n",
    "function_name = tool_call.function.name\n",
    "function_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "function_mapping = {\n",
    "    \"download_file_upload_to_assistant\": download_file_upload_to_assistant\n",
    "}\n",
    "\n",
    "print(\"Function Name:\", name)\n",
    "print(\"Function Arguments:\", arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfunction_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunction_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "function_mapping[function_name](**function_args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8b1e49fb13b48db535ed063883ad1a92f733d431d60de89a933f309bf130827"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
