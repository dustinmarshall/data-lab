{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERATE wb_ag_datasets.csv FILE ###\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "def is_related_to_food_or_agriculture(description):\n",
    "    keywords = {\n",
    "    'food', 'agriculture', 'farming', 'crops', 'livestock', 'harvest', \n",
    "    'horticulture', 'irrigation', 'fertilizer', 'farm', 'ranch', 'grain',\n",
    "    'vegetable', 'fruit', 'meat', 'dairy', 'poultry', 'aquaculture', 'agronomy',\n",
    "    'food security', 'rural development', 'agricultural economics', 'sustainable agriculture',\n",
    "    'agricultural policy', 'land use', 'agricultural trade', 'food supply', 'agroecology',\n",
    "    'agribusiness', 'agricultural technology', 'agricultural finance', 'agricultural investment',\n",
    "    'crop rotation', 'soil management', 'pest management', 'agricultural research', 'agricultural extension',\n",
    "    'food processing', 'food distribution', 'market access', 'subsistence agriculture', 'commercial agriculture',\n",
    "    'agricultural productivity', 'nutrition', 'food aid', 'agricultural innovation', 'climate change and agriculture',\n",
    "    'agricultural insurance', 'rural livelihoods', 'agricultural workers', 'agricultural supply chain',\n",
    "    'biofortification', 'food policy', 'agricultural sustainability', 'agroforestry'\n",
    "    }\n",
    "\n",
    "    if description is None:\n",
    "        return False\n",
    "\n",
    "    description_lower = description.lower()\n",
    "    for keyword in keywords:\n",
    "        if keyword in description_lower:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "dirname = os.getcwd()\n",
    "file_path = os.path.join(dirname, 'data/wb_datasets.json')\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "datasets = data['data']\n",
    "base_url = 'https://datacatalogapi.worldbank.org/ddhxext/DatasetView?dataset_unique_id='\n",
    "simplified_datasets = []\n",
    "\n",
    "for dataset in tqdm(datasets, desc=\"Processing datasets\"):\n",
    "    if dataset.get('source') == \"MICRODATA\":\n",
    "        continue\n",
    "    dataset_id = dataset.get('dataset_unique_id')\n",
    "    url = base_url + dataset_id\n",
    "    response = requests.get(url)\n",
    "    response_data = response.json()\n",
    "    name = response_data.get('name', 'None')\n",
    "    description = response_data.get('identification', {}).get('description', 'None')\n",
    "\n",
    "    if not is_related_to_food_or_agriculture(description):\n",
    "        continue\n",
    "\n",
    "    project_id = response_data.get('identification', {}).get('wb_project_reference', 'None')\n",
    "    resources = response_data.get('Resources', 'None')\n",
    "    \n",
    "    files = []\n",
    "    if resources != 'None':\n",
    "        for resource in resources:\n",
    "            file = {\n",
    "                'name': resource.get('name', 'None'),\n",
    "                'description': resource.get('description', 'None'),\n",
    "                'url': resource.get('url', 'None')\n",
    "            }\n",
    "            files.append(file)\n",
    "\n",
    "    simplified_dataset = {\n",
    "        'name': name,\n",
    "        'description': description,\n",
    "        'dataset_id': dataset_id,\n",
    "        'project_id': project_id,\n",
    "        'files': json.dumps(files)  # Convert list of files to a JSON string\n",
    "    }\n",
    "    simplified_datasets.append(simplified_dataset)\n",
    "\n",
    "output_file_path = os.path.join(dirname, 'data/wb_ag_datasets.csv')\n",
    "with open(output_file_path, mode='w', newline='', encoding='utf-8') as output_file:\n",
    "    writer = csv.DictWriter(output_file, fieldnames=['name', 'description', 'dataset_id', 'project_id', 'files'])\n",
    "    writer.writeheader()\n",
    "    for dataset in simplified_datasets:\n",
    "        writer.writerow(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERATE LIST OF VIDEO IDS FROM YOUTUBE CHANNEL ###\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Function to get summary text\n",
    "def get_summary(text):\n",
    "    # Load api key from env\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    client = OpenAI(\n",
    "        api_key=api_key,\n",
    "    )\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        print(\"Error: Text must be a string\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # Model can be changed if needed\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Summarize the following text in 40 tokens or less using the framing, 'A video on the World Bank's YouTube channel detailing {summary}'\"},\n",
    "                {\"role\": \"user\", \"content\": text}\n",
    "            ]\n",
    "        )\n",
    "        # Accessing the text in the correct format\n",
    "        return response.choices[0].message.content  # Adjusted to access .text attribute and strip any extra whitespace\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return None\n",
    "\n",
    "# Function to get video description\n",
    "def get_video_description(video_id):\n",
    "    # Load api key from env\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "    \n",
    "    # Build the YouTube API client\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Try to fetch the caption track list\n",
    "    try:\n",
    "        response = youtube.captions().list(\n",
    "            part='snippet',\n",
    "            videoId=video_id\n",
    "        ).execute()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching caption tracks: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Check if captions are available\n",
    "    if not response['items']:\n",
    "        print(\"No captions available for this video.\")\n",
    "        return None\n",
    "\n",
    "    # Try to fetch video details\n",
    "    try:\n",
    "        response = youtube.videos().list(\n",
    "            part='snippet',\n",
    "            id=video_id\n",
    "        ).execute()\n",
    "\n",
    "        # Check if the video exists and has a snippet\n",
    "        if not response['items']:\n",
    "            print(\"Video not found.\")\n",
    "            return None\n",
    "\n",
    "        video_details = response['items'][0]['snippet']\n",
    "\n",
    "        # Return the description\n",
    "        return video_details['description']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Function to get all video ids from a channel\n",
    "def get_all_video_ids(channel_id):\n",
    "    # Load api key from env\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "    # Initialize list\n",
    "    video_ids = []\n",
    "\n",
    "    # Get the channel's content details\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    channel_response = youtube.channels().list(\n",
    "        id=channel_id,\n",
    "        part='contentDetails'\n",
    "    ).execute()\n",
    "\n",
    "    # Get the playlist ID for the channel's videos\n",
    "    playlist_id = channel_response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "\n",
    "    # Get videos from the playlist\n",
    "    next_page_token = None\n",
    "    while True:\n",
    "        playlist_response = youtube.playlistItems().list(\n",
    "            playlistId=playlist_id,\n",
    "            part='contentDetails',\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        ).execute()\n",
    "\n",
    "        video_ids += [item['contentDetails']['videoId'] for item in playlist_response['items']]\n",
    "        \n",
    "        # Check if there is a next page\n",
    "        next_page_token = playlist_response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return video_ids\n",
    "\n",
    "# Example\n",
    "channel_id = 'UCE9mrcoX-oE-2f1BL-iPPoQ' # World Bank's Channel ID\n",
    "video_ids = get_all_video_ids(channel_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERATE EMBEDDINGS DICTIONARY FROM ALL VIDEOS ON YOUTUBE CHANNEL ###\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Group transcript entries (~10 words each) into groups of 20 with 20% overlap\n",
    "def group_transcript_entries(transcript):\n",
    "    grouped_transcripts = []\n",
    "    group_size = 20\n",
    "    overlap = 4\n",
    "\n",
    "    for i in range(0, len(transcript), group_size - overlap):\n",
    "        group = transcript[i:i + group_size]\n",
    "        group_text = \" \".join([entry['text'] for entry in group])\n",
    "        grouped_transcripts.append((group[0]['start'], group_text))\n",
    "\n",
    "    return grouped_transcripts\n",
    "\n",
    "# Format grouped transcripts into a list\n",
    "def format_transcript_to_list(grouped_transcripts, video_id, summary):\n",
    "    formatted_transcript = []\n",
    "    \n",
    "    for start_time, group_text in grouped_transcripts:\n",
    "        formatted_transcript.append({\"excerpt link\" : f\"https://www.youtube.com/watch?v={video_id}&t={int(start_time)}s\", \n",
    "                                     \"transcript excerpt\" : group_text,\n",
    "                                     \"video summary\" : summary})\n",
    "\n",
    "    return formatted_transcript\n",
    "\n",
    "# Extract transcipt\n",
    "def get_video_transcript(video_id):\n",
    "    # Load api key from env\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "    \n",
    "    # Build the YouTube API client\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Try to fetch the caption track list\n",
    "    try:\n",
    "        response = youtube.captions().list(\n",
    "            part='snippet',\n",
    "            videoId=video_id\n",
    "        ).execute()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching caption tracks: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Check if captions are available\n",
    "    if not response['items']:\n",
    "        print(\"No captions available for this video.\")\n",
    "        return None\n",
    "\n",
    "    # Extracting the transcript using the youtube_transcript_api\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "        # Grouping the transcript entries\n",
    "        grouped_transcripts = group_transcript_entries(transcript)\n",
    "        # Get video description to summarize\n",
    "        video_description = get_video_description(video_id)\n",
    "        # Get the summary\n",
    "        summary = get_summary(video_description)\n",
    "        # Formatting the grouped transcript to dictionary\n",
    "        formatted_transcript = format_transcript_to_list(grouped_transcripts, video_id, summary)\n",
    "        return formatted_transcript\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching transcript: {e}\")\n",
    "        return None\n",
    "\n",
    "# generate the transcripts list\n",
    "transcripts_list = []\n",
    "\n",
    "for id in tqdm(video_ids):\n",
    "    transcript_list = get_video_transcript(id)\n",
    "    if transcript_list:\n",
    "        transcripts_list += transcript_list\n",
    "\n",
    "# Saving the list to a new JSON file\n",
    "dirname = os.getcwd()\n",
    "output_file_path = os.path.join(dirname, 'data/wb_youtube_videos_complete.json')\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    json.dump(transcripts_list, output_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE wb_ag_projects_df FROM WB PROJECTS AND DOCUMENTS API ###\n",
    "\n",
    "# Function to create wb_ag_projects_df from WB Projects and Documents API\n",
    "def create_ag_projects_df():\n",
    "    \n",
    "    print(\"creating wb_ag_projects_df from WB Projects API...\")\n",
    "    wb_ag_projects = requests.get('https://search.worldbank.org/api/v2/projects?rows=10000&mjsectorcode_exact=AX').json()['projects']\n",
    "    wb_ag_projects_df = pd.DataFrame.from_dict(wb_ag_projects, orient='index')\n",
    "    \n",
    "    print(\"adding projectdocs from to WB Documents API wb_ag_projects_df...\")\n",
    "    for index, row in tqdm(wb_ag_projects_df.iterrows(), total=wb_ag_projects_df.shape[0]):\n",
    "        wb_ag_projects_df.at[index, 'projectdocs'] = {}\n",
    "        response = requests.get(f\"https://search.worldbank.org/api/v2/wds?format=json&fl=pdfurl,docty&proid={row['id']}\").json()\n",
    "        if response:\n",
    "            for doc in reversed(response['documents'].values()):\n",
    "                if 'docty' in doc and 'pdfurl' in doc:\n",
    "                    doctype = doc['docty']\n",
    "                    pdfurl = doc['pdfurl']\n",
    "                    wb_ag_projects_df.at[index, 'projectdocs'][doctype] = pdfurl\n",
    "\n",
    "    print(\"saving wb_ag_projects_df to csv...\")\n",
    "    wb_ag_projects_df.to_csv('data/wb_ag_projects.csv', index=False)\n",
    "\n",
    "# Check if wb_ag_projects.csv exists, if not create it\n",
    "if not os.path.exists('data/wb_ag_projects.csv'):\n",
    "    create_ag_projects_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERATE ASSISTANT FOR GENERATING USE CASES ###\n",
    "\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "\n",
    "csv_input = 'data/wb_ag_ext_projects.csv'\n",
    "csv_output = 'data/wb_ag_ext_usecases.csv'\n",
    "\n",
    "# Function to submit tool outputs\n",
    "def submit_tool_outputs(thread_id, run_id, tool_call_id, output):\n",
    "    client.beta.threads.runs.submit_tool_outputs(\n",
    "        thread_id=thread_id,\n",
    "        run_id=run_id,\n",
    "        tool_outputs=[{\n",
    "            \"tool_call_id\": tool_call_id,\n",
    "            \"output\": json.dumps(output)\n",
    "        }]\n",
    "    )\n",
    "\n",
    "# Function to process a document and update the DataFrame\n",
    "def process_document(url, id, project, implementer, region, country, documents, sectors, years, contacts):\n",
    "    \n",
    "    # Load api key from env\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    # Read wb_use_case_summaries.csv into wb_use_case_summaries_df\n",
    "    if os.path.exists(csv_output):\n",
    "        use_cases_df = pd.read_csv(csv_output)\n",
    "    else:\n",
    "        use_cases_df = pd.DataFrame(columns=['id', 'use_case', 'project', 'description', 'implementer', 'region', 'country', 'documents', 'sectors', 'years', 'contacts'])\n",
    "\n",
    "    # Download the file\n",
    "    response = requests.get(url)\n",
    "    file_name = 'downloaded_file.pdf'\n",
    "    with open(file_name, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "    # Initialize OpenAI client\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    # Upload the file to OpenAI\n",
    "    print(\"Uploading files...\")\n",
    "    file_path = file_name\n",
    "    project_document_file = client.files.create(\n",
    "      file=open(file_path, \"rb\"),\n",
    "      purpose='assistants'\n",
    "    )\n",
    "    \n",
    "    # Initialize tools\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"retrieval\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Function mapping\n",
    "    function_mapping = {\n",
    "    }\n",
    "    \n",
    "    # Create the Assistant\n",
    "    print(\"Creating Assistant...\")\n",
    "    assistant = client.beta.assistants.create(\n",
    "        name=\"Use Case Summarizer\",\n",
    "        instructions=\n",
    "            '''\n",
    "            Role: \n",
    "            In your knowledge base is a pdf file detailing an agricultural project. Your job is to identify the distinct methodologies that\n",
    "            the project leverages to implement the project, then produce a summary of each methodology (framing the methodology as a \n",
    "            use case example) in alignment with the provided template. The goal is to produce use case examples that can be used by other \n",
    "            teams in their own projects.\n",
    "\n",
    "            Instructions:\n",
    "            - Review the document in your knowledge base and identify the distinct methodologies that the project leverages.\n",
    "            - For each, generate a 200 word description and 5-10 word title detailing how the use case example was implemented.\n",
    "            - Return the title and description in a JSON array, with no additional text before or after.\n",
    "            - Ensure that the description is actually 200 words and the title is actually 5-10 words.\n",
    "            - Even if methodologies aren't explicitly mentioned, do your best to deduce them, otherwise return an empty JSON array.\n",
    "            \n",
    "            Template:\n",
    "            [\n",
    "                {\n",
    "                    \"use_case\": \"Title of use case example 1\",\n",
    "                    \"description\": \"Description of use case example 1\"\n",
    "                },\n",
    "                {\n",
    "                    \"use_case\": \"Title of use case example 2\",\n",
    "                    \"description\": \"Description of use case example 2\"\n",
    "                }\n",
    "            ]\n",
    "            ''',\n",
    "        model=\"gpt-4-0125-preview\",\n",
    "        tools = tools,\n",
    "        file_ids=[project_document_file.id]\n",
    "    )\n",
    "\n",
    "    # Create a thread\n",
    "    print(\"Creating thread...\")\n",
    "    thread = client.beta.threads.create(\n",
    "    )\n",
    "\n",
    "    # Add message to the thread\n",
    "    print(\"Adding message to the thread...\")\n",
    "    message = client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content= \"\"\n",
    "    )\n",
    "\n",
    "    # Run the Assistant\n",
    "    print(\"Running the Assistant...\")\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id, \n",
    "        assistant_id=assistant.id,\n",
    "        instructions=\"\"\n",
    "    )\n",
    "\n",
    "    # Handle tool outputs\n",
    "    while run.status != 'completed':\n",
    "        time.sleep(10)\n",
    "        \n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id\n",
    "        )\n",
    "        \n",
    "        # Print the run status\n",
    "        print(f\"Run status: {run.status}\")\n",
    "        \n",
    "        # If the run is failed, retry\n",
    "        if run.status == \"failed\":\n",
    "            print(\"Run failed:\", run.last_error)\n",
    "            break\n",
    "\n",
    "        # Handle the run status\n",
    "        if run.status == \"requires_action\":\n",
    "            print(\"Action required by the assistant...\")\n",
    "            for tool_call in run.required_action.submit_tool_outputs.tool_calls:\n",
    "                if tool_call.type == \"function\":\n",
    "                    function_name = tool_call.function.name\n",
    "                    print(f\"Function name: {function_name}\")\n",
    "                    arguments = json.loads(tool_call.function.arguments)\n",
    "                    print(f\"Arguments: {arguments}\")\n",
    "                    if function_name in function_mapping:\n",
    "                        print(f\"Calling function {function_name}...\")\n",
    "                        response = function_mapping[function_name](**arguments)\n",
    "                        submit_tool_outputs(thread.id, run.id, tool_call.id, response)\n",
    "\n",
    "    # Fetch the Assistant's response  \n",
    "    print(\"Fetching Assistant's response...\")\n",
    "    reply = client.beta.threads.messages.list(\n",
    "        thread_id=thread.id\n",
    "    )\n",
    "    messages = reply.data\n",
    "\n",
    "    assistant_reply = \"\"\n",
    "    for message in messages:\n",
    "        if message.role == \"assistant\":\n",
    "            for content in message.content:\n",
    "                if content.type == \"text\":\n",
    "                    assistant_reply = content.text.value\n",
    "                    print(\"assistant_reply: \", assistant_reply, \"type: \", type(assistant_reply))\n",
    "                    # Find the position of the first '[' and the last ']'\n",
    "                    start_index = assistant_reply.find('[')\n",
    "                    end_index = assistant_reply.rfind(']')\n",
    "\n",
    "                    # Extract the substring between these positions\n",
    "                    if start_index != -1 and end_index != -1:\n",
    "                        json_string = assistant_reply[start_index:end_index+1]\n",
    "                        try:\n",
    "                            formatted_assistant_reply = json.loads(json_string)\n",
    "                            print(\"formatted_assistant_reply: \", formatted_assistant_reply, \"type: \", type(formatted_assistant_reply))\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(\"Failed to decode JSON. Error: \", e)\n",
    "                    else:\n",
    "                        print(\"The string does not contain a valid JSON structure.\")\n",
    "\n",
    "                    break\n",
    "            if assistant_reply:\n",
    "                break\n",
    "    \n",
    "    # Process the Assistant's response\n",
    "    for use_case in formatted_assistant_reply:\n",
    "        \n",
    "        print(\"use case: \", use_case, \"type: \", type(use_case))\n",
    "        \n",
    "        # Check if the use_case is a string and convert it to a dictionary\n",
    "        if isinstance(use_case, str):\n",
    "            use_case = ast.literal_eval(use_case)\n",
    "        \n",
    "        # Add additional fields to the use_case\n",
    "        use_case['id'] = id\n",
    "        use_case['project'] = project\n",
    "        use_case['implementer'] = implementer\n",
    "        use_case['region'] = region\n",
    "        use_case['country'] = country\n",
    "        use_case['documents'] = documents\n",
    "        use_case['sectors'] = sectors\n",
    "        use_case['years'] = years\n",
    "        use_case['contacts'] = contacts\n",
    "\n",
    "        # Add new row to DataFrame\n",
    "        use_cases_df = pd.concat([use_cases_df, pd.DataFrame([use_case])], ignore_index=True)\n",
    "\n",
    "        # Write the updated DataFrame to CSV\n",
    "        use_cases_df.to_csv(csv_output, index=False)\n",
    "\n",
    "    # Delete the file locally\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "    else:\n",
    "        print(\"The file does not exist\")\n",
    "        \n",
    "    ## Delete the file object\n",
    "    #client.beta.assistants.files.delete(\n",
    "    #    file_id=project_document_file.id,\n",
    "    #    assistant_id=assistant.id\n",
    "    #)\n",
    "        \n",
    "    ## Delete the assistant\n",
    "    #client.beta.assistants.delete(\n",
    "    #    assistant_id=assistant.id\n",
    "    #)\n",
    "\n",
    "# Read wb_ag_projects.csv into wb_ag_projects_df\n",
    "projects_df = pd.read_csv(csv_input)\n",
    "\n",
    "# Read wb_use_case_summaries.csv into wb_use_case_summaries_df\n",
    "if os.path.exists(csv_output):\n",
    "    use_cases_df = pd.read_csv(csv_output)\n",
    "else:\n",
    "    use_cases_df = pd.DataFrame(columns=['id', 'use_case', 'project', 'description', 'implementer', 'region', 'country', 'documents', 'sectors', 'years', 'contacts'])\n",
    "\n",
    "# iterate through wb_ag_projects_df and process each document\n",
    "for index, row in projects_df.iterrows():\n",
    "    if not use_cases_df['id'].isin([row['id']]).any():\n",
    "        # Check if projectdocs is a string and convert it to a dictionary\n",
    "        if isinstance(row['documents'], str):\n",
    "            try:\n",
    "                documents = ast.literal_eval(row['documents'])\n",
    "            except ValueError:\n",
    "                # Handle the exception if the string cannot be converted to a dictionary\n",
    "                continue\n",
    "        for doctype, url in documents.items():\n",
    "            if doctype == 'Project Paper' or doctype == 'Implementation Completion and Results Report' or doctype == 'Implementation Completion Report Review': \n",
    "                print(\"Processing document: \", doctype, \" for project: \", row['id'])\n",
    "                process_document(url, row['id'], row['project'], row['implementer'], row['region'], row['country'], row['documents'], row['sectors'], row['years'], row['contacts'])\n",
    "                #time.sleep(300)\n",
    "                break\n",
    "            elif doctype == 'Project Appraisal Document' or doctype == 'Project Information Document':\n",
    "                print(\"Processing document: \", doctype, \" for project: \", row['id'])\n",
    "                process_document(url, row['id'], row['project'], row['implementer'], row['region'], row['country'], row['documents'], row['sectors'], row['years'], row['contacts'])\n",
    "                #time.sleep(300)\n",
    "                break\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### DELETE ASSISTANTS AND FILES ###\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      5\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "### DELETE ASSISTANTS AND FILES ###\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# Load api key from env\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "my_assistants = client.beta.assistants.list(\n",
    "    limit=\"100\"\n",
    ")\n",
    "\n",
    "# Delete all assistants and files\n",
    "for assistant in my_assistants:\n",
    "    \n",
    "    # # Delete all files\n",
    "    try:\n",
    "        my_files = client.beta.assistants.files.list(\n",
    "            \n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching files list:\", e)\n",
    "        my_files = []\n",
    "    for file in my_files:\n",
    "        print(\"Deleting file ID:\", file.id)\n",
    "        try:\n",
    "            client.beta.assistants.files.delete(\n",
    "                file_id=file.id,\n",
    "                assistant_id=file.assistant_id\n",
    "            )\n",
    "            print(\"File deleted successfully.\")\n",
    "        except Exception as e:\n",
    "            print(\"Error deleting file:\", e)\n",
    "    \n",
    "    # Delete the assistant\n",
    "    try:\n",
    "        client.beta.assistants.delete(\n",
    "            assistant_id=assistant.id\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting the index...\n",
      "Creating the index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing use case examples...: 100%|██████████| 50/50 [00:23<00:00,  2.16it/s]\n",
      "Processing learning materials...: 100%|██████████| 25/25 [00:09<00:00,  2.53it/s]\n",
      "Processing datasets...: 100%|██████████| 25/25 [00:09<00:00,  2.77it/s]\n"
     ]
    }
   ],
   "source": [
    "### GENERATE PINECONE VECTOR DATABASE AND EMBEDDINGS FOR USE CASES ###\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import streamlit as st\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import time\n",
    "import ast\n",
    "\n",
    "# Load api key from env\n",
    "pinecone_api_key = st.secrets[\"PINECONE_API_KEY\"]\n",
    "api_key = st.secrets[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Delete the index\n",
    "print(\"Deleting the index...\")\n",
    "pc.delete_index(\"agrifooddatalab-index\")\n",
    "\n",
    "# Create the index\n",
    "print(\"Creating the index...\")\n",
    "pc.create_index(\n",
    "    name=\"agrifooddatalab-index\",\n",
    "    dimension=1536,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(\n",
    "        cloud='aws', \n",
    "        region='us-west-2'\n",
    "    ) \n",
    ") \n",
    "\n",
    "# Connect to the index\n",
    "index = pc.Index(\"agrifooddatalab-index\")\n",
    "\n",
    "# Function to get embeddings\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    try:\n",
    "        return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Load wb_ag_ext_usecases.csv file\n",
    "dirname = os.getcwd()\n",
    "file_path = os.path.join(dirname, 'data/wb_ag_usecases.csv')\n",
    "\n",
    "with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for i, entry in enumerate(tqdm(list(reader)[:50], desc=\"Processing use case examples...\")):\n",
    "        # Extracting the text\n",
    "        embedding_text = f\"Title: {entry['use_case']}.\\nDescription: {entry['description']}\"\n",
    "        # Get the embedding\n",
    "        embedding = get_embedding(embedding_text)\n",
    "\n",
    "        # Try to upsert up to three times with a wait time between retries\n",
    "        max_retries = 3\n",
    "        wait_time = 5  # wait time in seconds\n",
    "\n",
    "        # generate the use case id, starting with U00001\n",
    "        use_case_id = \"U\" + f'{i+1:05}'\n",
    "                \n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                # Insert the embedding into the index\n",
    "                index.upsert([\n",
    "                    (\n",
    "                        use_case_id, \n",
    "                        embedding, \n",
    "                        {\n",
    "                            'title': entry['use_case'],\n",
    "                            'description': entry['description'],\n",
    "                            'text_to_insert': f\"**{entry['use_case']} in {entry['country']}:** {entry['description'][:entry['description'].find('.')+1]} (ID: {use_case_id})\",\n",
    "                            'type': 'use case',\n",
    "                            'project': entry['project'],\n",
    "                            'implementer': entry['implementer'],\n",
    "                            'region': entry['region'],\n",
    "                            'country': entry['country'],\n",
    "                            'document(s)': ast.literal_eval(entry['documents']),\n",
    "                            'subtopic(s)': ast.literal_eval(entry['sectors']),\n",
    "                            'year(s)': ast.literal_eval(entry['years']),\n",
    "                            'contact(s)': ast.literal_eval(entry['contacts']),\n",
    "                            'project_id': entry['id'],\n",
    "                        }\n",
    "                    )\n",
    "                ])\n",
    "                break  # If upsert is successful, break out of the retry loop\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt} failed: {e}\")\n",
    "                if attempt < max_retries:\n",
    "                    print(f\"Waiting for {wait_time} seconds before retrying...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(\"Max retries reached. Moving to the next item.\")\n",
    "             \n",
    "# Load wb_ag_ext_papers.csv file       \n",
    "file_path = os.path.join(dirname, 'data/wb_ag_ext_papers.csv')\n",
    "\n",
    "with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for entry in tqdm(list(reader)[:25], desc=\"Processing learning materials...\"):\n",
    "        # Extracting the text\n",
    "        embedding_text = f\"Title: {entry['document']}.\\nDescription: {entry['abstract']}\"\n",
    "        # Get the embedding\n",
    "        embedding = get_embedding(embedding_text)\n",
    "\n",
    "        # Try to upsert up to three times with a wait time between retries\n",
    "        max_retries = 3\n",
    "        wait_time = 5  # wait time in seconds\n",
    "\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                # Insert the embedding into the index\n",
    "                index.upsert([\n",
    "                    (\n",
    "                        entry['id'], \n",
    "                        embedding, \n",
    "                        {\n",
    "                            'title': entry['document'],\n",
    "                            'description': entry['abstract'],\n",
    "                            'type': 'learning',\n",
    "                            'date': entry['date'],\n",
    "                            'author(s)': ast.literal_eval(entry['authors']),\n",
    "                            'sector(s)': ast.literal_eval(entry['sectors']),\n",
    "                            'implementer': entry['implementer'],\n",
    "                            'url': entry['url']\n",
    "                        }\n",
    "                    )\n",
    "                ])\n",
    "                break  # If upsert is successful, break out of the retry loop\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt} failed: {e}\")\n",
    "                if attempt < max_retries:\n",
    "                    print(f\"Waiting for {wait_time} seconds before retrying...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(\"Max retries reached. Moving to the next item.\")\n",
    "                    \n",
    "# Load wb_ag_ext_datasets.csv file       \n",
    "file_path = os.path.join(dirname, 'data/wb_ag_datasets.csv')\n",
    "\n",
    "with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for entry in tqdm(list(reader)[:25], desc=\"Processing datasets...\"):\n",
    "        # Extracting the text\n",
    "        embedding_text = f\"Title: {entry['name']}.\\nDescription: {entry['description']}\"\n",
    "        # Get the embedding\n",
    "        embedding = get_embedding(embedding_text)\n",
    "\n",
    "        # Try to upsert up to three times with a wait time between retries\n",
    "        max_retries = 3\n",
    "        wait_time = 5  # wait time in seconds\n",
    "\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                # Insert the embedding into the index\n",
    "                index.upsert([\n",
    "                    (\n",
    "                        entry['dataset_id'], \n",
    "                        embedding, \n",
    "                        {\n",
    "                            'title': entry['name'],\n",
    "                            'description': entry['description'],\n",
    "                            'type': 'dataset',\n",
    "                            'project_id': entry['project_id'],\n",
    "                            'file(s)': ast.literal_eval(entry['files'])\n",
    "                        }\n",
    "                    )\n",
    "                ])\n",
    "                break  # If upsert is successful, break out of the retry loop\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt} failed: {e}\")\n",
    "                if attempt < max_retries:\n",
    "                    print(f\"Waiting for {wait_time} seconds before retrying...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(\"Max retries reached. Moving to the next item.\")\n",
    "\n",
    "# Example: Querying the index\n",
    "# query_result = index.query(queries=[[0.1, 0.2, ..., 0.128]], top_k=5)\n",
    "\n",
    "# Remember to delete the index if it is no longer needed\n",
    "#pinecone.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ukraine',\n",
       " 'Kosovo',\n",
       " 'Mauritius',\n",
       " 'Tunisia',\n",
       " 'Western and Central Africa',\n",
       " 'Chad',\n",
       " 'Philippines',\n",
       " 'Lebanon',\n",
       " 'Afghanistan',\n",
       " 'Ghana',\n",
       " 'Central African Republic',\n",
       " 'Turkiye',\n",
       " 'Kazakhstan',\n",
       " 'Morocco',\n",
       " 'China',\n",
       " 'Moldova',\n",
       " 'Eastern and Southern Africa']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import ast\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/wb_ag_usecases.csv')\n",
    "\n",
    "# Extract the unique implementers from the 'implementer' column\n",
    "unique_implementers = df['implementer'].unique().tolist()\n",
    "unique_implementers\n",
    "\n",
    "# Extract the unique regions from the 'region' column\n",
    "unique_regions = df['region'].unique().tolist()\n",
    "unique_regions\n",
    "\n",
    "# Extract the unique years from the 'years' column\n",
    "df['years'] = df['years'].apply(ast.literal_eval)\n",
    "unique_years = df['years'].explode().unique().tolist()\n",
    "unique_years\n",
    "\n",
    "# Extract the unique sectors from the lists of sectors in the 'sectors' column\n",
    "df['sectors'] = df['sectors'].apply(ast.literal_eval)\n",
    "unique_sectors = df['sectors'].explode().unique().tolist()\n",
    "unique_sectors\n",
    "\n",
    "# Extract the unique countries from the 'country' column\n",
    "unique_countries = df['country'].unique().tolist()\n",
    "unique_countries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contact(s)': ['Sergiy Zorya', 'Johanna Jaeger'],\n",
       " 'country': 'Ukraine',\n",
       " 'description': 'This use case example reflects a strategic implementation of accessible financial support mechanisms to bolster agricultural productivity in the face of adversity. In the context of the Ukraine Agriculture Recovery Inclusive Support Emergency (ARISE) Project, financial accessibility was addressed through a two-pronged approach aimed at enhancing the resilience and sustainability of the agricultural sector amidst significant challenges, including conflict and natural or man-made disasters. Firstly, affordable credit was made available to farms, facilitating capital investment and operational continuity. This component targeted the broader agricultural ecosystem, ensuring entities of varying sizes could access the financial resources necessary for recovery and future growth. Secondly, the project instituted a grant system specifically tailored for small farms, recognizing the disproportionate impact of crises on smaller scale operators and the vital role these entities play in local food security and rural economies. Both methods were instrumented through the Business Development Fund and the Ministry of Agrarian Policy and Food, ensuring tailored, effective deployment of resources. This dual approach not only aimed at immediate recovery needs but also at fostering a more resilient agricultural infrastructure capable of withstanding future crises, thereby contributing to the broader objective of maintaining inclusive agricultural production.',\n",
       " 'document(s)': {'Environmental and Social Commitment Plan': 'http://documents.worldbank.org/curated/en/099101323081517746/pdf/P1807320b55dce08094c805edd1c25d291.pdf',\n",
       "  'Project Appraisal Document': 'http://documents.worldbank.org/curated/en/099101923095537231/pdf/BOSIB0ba93e6360030a1f50cd2b8325e6e5.pdf',\n",
       "  'Loan Agreement': 'http://documents.worldbank.org/curated/en/099110823104018617/pdf/P180732029a3570450a99a0bd5c5b50c75e.pdf',\n",
       "  'Disbursement Letter': 'http://documents.worldbank.org/curated/en/099012224102528602/pdf/P1807321506d0708c1bfaa1a4e9704ee164.pdf',\n",
       "  'Grant or Trust Fund Agreement': 'http://documents.worldbank.org/curated/en/099110823104512771/pdf/P18073205291e4085082aa0f5beef254bd0.pdf',\n",
       "  'Minutes': 'http://documents.worldbank.org/curated/en/099111423164011650/pdf/BOSIB086de382c0e608db20aeb697ce1ee6.pdf',\n",
       "  'Letter': 'http://documents.worldbank.org/curated/en/099121923092528294/pdf/P1807320a5970509a0b66a0d45aa616f92a.pdf'},\n",
       " 'implementer': 'Ministry of Agrarian Policy and Food, Business Development Fund',\n",
       " 'project': 'Ukraine Agriculture Recovery Inclusive Support Emergency (ARISE) Project',\n",
       " 'project_id': 'P180732',\n",
       " 'region': 'Europe and Central Asia',\n",
       " 'subtopic(s)': ['Agricultural Extension, Research, and Other Support Activities'],\n",
       " 'title': 'Accessible Agricultural Financing',\n",
       " 'type': 'use case',\n",
       " 'year(s)': ['2023', '2024', '2025']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=st.secrets[\"PINECONE_API_KEY\"], environment=\"gcp-starter\")\n",
    "\n",
    "index = pc.Index(\"agrifooddatalab-index\")\n",
    "\n",
    "def get_record(id):\n",
    "    key = list(index.fetch(ids=[id])['vectors'])[0]\n",
    "    metadata = index.fetch(ids=[id])['vectors'][key]['metadata']\n",
    "    metadata.pop('short_description', None)\n",
    "    metadata.pop('text_to_insert', None)\n",
    "    documents = {}\n",
    "    for doc in metadata['document(s)']:\n",
    "        doc = ast.literal_eval('{' + doc + '}')\n",
    "        documents.update(doc)\n",
    "    metadata['document(s)'] = documents\n",
    "    return metadata\n",
    "\n",
    "get_record(\"U00001\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8b1e49fb13b48db535ed063883ad1a92f733d431d60de89a933f309bf130827"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
